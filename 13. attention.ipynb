{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google Colab utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # memory footprint support libraries/code\n",
    "# !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "# !pip install gputil\n",
    "# !pip install psutil\n",
    "# !pip install humanize\n",
    "# import psutil\n",
    "# import humanize\n",
    "# import os\n",
    "# import GPUtil as GPU\n",
    "# GPUs = GPU.getGPUs()\n",
    "# # XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
    "# gpu = GPUs[0]\n",
    "# def printm():\n",
    "#  process = psutil.Process(os.getpid())\n",
    "#  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "#  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "# printm() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!kill -9 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/gdrive')\n",
    "# %cd \"/gdrive/My Drive/air-pollution\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Lambda, Reshape, Dropout\n",
    "from tensorflow.keras.layers import Bidirectional, RepeatVector, Dot, Activation\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import mse\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from kerastuner import HyperModel\n",
    "from kerastuner.tuners import RandomSearch\n",
    "from utils import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoder_input_data = np.load('./data/third-order/seq2seq/train_encoder_input_data.npy')\n",
    "train_decoder_input_data = np.load('./data/third-order/seq2seq/train_decoder_input_data.npy')\n",
    "train_decoder_target_data = np.load('./data/third-order/seq2seq/train_decoder_target_data.npy')\n",
    "\n",
    "valid_encoder_input_data = np.load('./data/third-order/seq2seq/valid_encoder_input_data.npy')\n",
    "valid_decoder_input_data = np.load('./data/third-order/seq2seq/valid_decoder_input_data.npy')\n",
    "valid_decoder_target_data = np.load('./data/third-order/seq2seq/valid_decoder_target_data.npy')\n",
    "\n",
    "test_encoder_input_data = np.load('./data/third-order/seq2seq/test_encoder_input_data.npy')\n",
    "test_decoder_input_data = np.load('./data/third-order/seq2seq/test_decoder_input_data.npy')\n",
    "test_decoder_target_data = np.load('./data/third-order/seq2seq/test_decoder_target_data.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to the model architecture, we need to transform the output shape and type\n",
    "train_decoder_target_data = list(np.swapaxes(train_decoder_target_data, 0, 1))\n",
    "valid_decoder_target_data = list(np.swapaxes(valid_decoder_target_data, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_encoder_input_data.shape)\n",
    "print(train_decoder_input_data.shape)\n",
    "print(len(train_decoder_target_data), train_decoder_target_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tx, encoder_input_dim = (train_encoder_input_data.shape[1], \n",
    "                         train_encoder_input_data.shape[2])\n",
    "    \n",
    "Ty, decoder_input_dim = (train_decoder_input_data.shape[1], \n",
    "                         train_decoder_input_data.shape[2])\n",
    "\n",
    "# we are predicting the pollution only, leave out the mask\n",
    "decoder_output_dim = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 250\n",
    "max_trials = 250\n",
    "executions_per_trial = 1\n",
    "patience = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(encoder_outputs, h_prev, attention_repeat, \n",
    "                       attention_concatenate, attention_dense_1,\n",
    "                       attention_dense_2, attention_activation,\n",
    "                       attention_dot):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed \n",
    "    as a dot product of the attention weights \"alphas\" and the outputs\n",
    "    of the encoder.\n",
    "    \n",
    "    Arguments:\n",
    "    encoder_outputs -- outputs of the encoder, numpy-array of shape \n",
    "                       (m, Tx, 2*encoder_latent_dim)\n",
    "    h_prev -- previous hidden state of the decoder LSTM, numpy-array \n",
    "              of shape (m, decoder_latent_dim)\n",
    "    attention_repeat -- predefined repeat layer for the attention\n",
    "    attention_concatenate -- predefined concatenate layer for the \n",
    "                             attention\n",
    "    attention_dense_1 -- predefined dense layer for the attention\n",
    "    attention_dense_2 -- predefined dense layer for the attention\n",
    "    attention_activation -- predefined activation layer for the \n",
    "                            attention\n",
    "    attention_dot -- predefined dot layer for the attention\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input to the decoder LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    # Repeat h_prev to be of shape (m, Tx, decoder_latent_dim) \n",
    "    h_prev = attention_repeat(h_prev)\n",
    "    \n",
    "    # Concatenate the encoder outputs to the repeated vector\n",
    "    concat = attention_concatenate([encoder_outputs, h_prev])\n",
    "       \n",
    "    # Compute the energies\n",
    "    energies = attention_dense_1(concat)\n",
    "    energies = attention_dense_2(energies)\n",
    "\n",
    "    # Compute the alphas by applying softmax on the energies\n",
    "    alphas = attention_activation(energies)\n",
    "\n",
    "    # Compute the context vector by dotting the alphas and \n",
    "    # the encoder outputs\n",
    "    context = attention_dot([alphas, encoder_outputs])\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionModel(HyperModel):\n",
    "\n",
    "    def __init__(self, Tx, Ty, encoder_input_dim, decoder_input_dim, \n",
    "                 decoder_output_dim):\n",
    "        \"\"\"\n",
    "        Constructor for the derived HyperModel class\n",
    "        \n",
    "        Arguments:\n",
    "        Tx -- length of the input sequence\n",
    "        Ty -- length of the output sequence\n",
    "        encoder_input_dim -- length of input vector for the encoder\n",
    "        decoder_input_dim -- length of input vector for the decoder\n",
    "        decoder_output_dim -- length of output vector for the decoder\n",
    "        \"\"\"\n",
    "        self.Tx = Tx\n",
    "        self.Ty = Ty\n",
    "        self.encoder_input_dim = encoder_input_dim\n",
    "        self.decoder_input_dim = decoder_input_dim\n",
    "        self.decoder_output_dim = decoder_output_dim\n",
    "\n",
    "\n",
    "    def build(self, hp):\n",
    "        \"\"\"\n",
    "        Builds a seq2seq LSTM model with an attention mechanism. \n",
    "\n",
    "        Arguments:\n",
    "        hp -- hyperparameters object from keras-tuner\n",
    "\n",
    "        Returns:\n",
    "        model -- Keras model instance\n",
    "        \"\"\"\n",
    "\n",
    "        # ------------------- SHARED LAYERS ---------------------\n",
    "        encoder_latent_dim = hp.Int('encoder_latent_dim', min_value=16, max_value=48, \n",
    "                                    step=16)\n",
    "        decoder_latent_dim = 2 * encoder_latent_dim\n",
    "        attention_dense_dim = hp.Int('attention_dense_dim', min_value=8, max_value=14, \n",
    "                                     step=2)\n",
    "\n",
    "        # Encoder layers\n",
    "        encoder_lstm = Bidirectional(LSTM(encoder_latent_dim, return_sequences=True, \n",
    "                                          name='encoder_lstm'), merge_mode='concat')\n",
    "\n",
    "        # Attention layers\n",
    "        attention_repeat = RepeatVector(self.Tx, name='attention_repeat')\n",
    "        attention_concatenate = Concatenate(axis=-1, name='attention_concatenate')\n",
    "        attention_dense_1 = Dense(attention_dense_dim, activation='tanh', \n",
    "                                  name='attention_dense_1')\n",
    "        attention_dense_2 = Dense(1, activation='relu', name='attention_dense_2')\n",
    "        attention_activation = Activation(softmax, name='attention_activation') \n",
    "        attention_dot = Dot(axes = 1)\n",
    "\n",
    "        # Decoder layers\n",
    "        decoder_concatenate = Concatenate(axis=-1, name='decoder_concatenate')\n",
    "        decoder_lstm = LSTM(decoder_latent_dim, return_state=True, \n",
    "                            name='decoder_lstm')\n",
    "        decoder_dense = Dense(self.decoder_output_dim, activation='linear',\n",
    "                              name='decoder_dense')\n",
    "\n",
    "        # ---------------------- MODEL ------------------------\n",
    "        # Encoder inputs\n",
    "        encoder_inputs = Input(shape=(self.Tx, self.encoder_input_dim), \n",
    "                               name='encoder_input')\n",
    "        encoder_outputs = encoder_lstm(encoder_inputs)\n",
    "\n",
    "        # Decoder inputs\n",
    "        decoder_inputs = Input(shape=(self.Ty, self.decoder_input_dim), \n",
    "                               name='decoder_input')\n",
    "\n",
    "        # Zeros tensors as initial values for h and c.\n",
    "        # Basically, I apply the decoder LSTM on the first timestep of encoder outputs  \n",
    "        # concatenated with decoder inputs in order to get the hidden states h and c, \n",
    "        # and then I create zeros tensors from their shape, because I cannot obtain \n",
    "        # the batch size dynamically. Moreover, I have to apply an identity lambda \n",
    "        # function in order to cast the zero tensor to a Keras tensor (otherwise it \n",
    "        # cannot be passed as initial_state)\n",
    "\n",
    "        # x is a slice of the encoder outputs\n",
    "        x = Lambda(lambda x: x[:, 0, :])(encoder_outputs)\n",
    "        x = K.expand_dims(x, axis=1)\n",
    "        # y is a slice of the decoder inputs\n",
    "        y = Lambda(lambda y: y[:, 0, :])(decoder_inputs)\n",
    "        y = K.expand_dims(y, axis=1)\n",
    "        # z is a concatenation of x and y\n",
    "        z = Concatenate(axis=-1)([x, y])\n",
    "        # we feed the dummy tensor in order to obtain a sample tensor of h and c\n",
    "        _, h, c = decoder_lstm(z)\n",
    "        # create tensors of zeros using the shapes of the previous dummies\n",
    "        h = Lambda(lambda x: x, name='h0')(K.zeros_like(h))\n",
    "        c = Lambda(lambda x: x, name='c0')(K.zeros_like(c))\n",
    "\n",
    "        # Decoder outputs\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(self.Ty):\n",
    "            # Perform attention for one timestep\n",
    "            context = one_step_attention(encoder_outputs, h, attention_repeat, \n",
    "                                         attention_concatenate, attention_dense_1,\n",
    "                                         attention_dense_2, attention_activation,\n",
    "                                         attention_dot)\n",
    "            \n",
    "            # Concatenate the context vector and the decoder input\n",
    "            decoder_input = Lambda(lambda x: x[:, t, :])(decoder_inputs)\n",
    "            decoder_input = K.expand_dims(decoder_input, axis=1)\n",
    "            full_decoder_input = decoder_concatenate([context, decoder_input])\n",
    "\n",
    "            # Apply post-attention LSTM\n",
    "            h, _, c = decoder_lstm(inputs=full_decoder_input, initial_state=[h, c])\n",
    "\n",
    "            # Apply dense to compute the output\n",
    "            decoder_output = decoder_dense(h)\n",
    "            \n",
    "            # Append the the model's outputs\n",
    "            outputs.append(decoder_output)\n",
    "\n",
    "        model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "        optimizer = Adam(learning_rate=hp.Float('learning_rate', 1e-4, 1e-2, \n",
    "                                                sampling='log'))\n",
    "        model.compile(optimizer=optimizer, loss=attention_masked_mse)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_builder = AttentionModel(Tx, Ty, encoder_input_dim, decoder_input_dim,\n",
    "                                decoder_output_dim)\n",
    "tuner = RandomSearch(model_builder,\n",
    "                     objective='val_loss',\n",
    "                     max_trials=max_trials,\n",
    "                     executions_per_trial=executions_per_trial,\n",
    "                     directory='local-keras-tuner/attention', \n",
    "                     project_name='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tuner.search(x=[train_encoder_input_data, \n",
    "                train_decoder_input_data], \n",
    "             y=train_decoder_target_data,\n",
    "             validation_data=([\n",
    "                valid_encoder_input_data,\n",
    "                valid_decoder_input_data],\n",
    "                valid_decoder_target_data),\n",
    "             batch_size=batch_size,\n",
    "             epochs=epochs,\n",
    "             callbacks=[EarlyStopping(monitor='val_loss', \n",
    "                                      patience=patience, \n",
    "                                      verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
